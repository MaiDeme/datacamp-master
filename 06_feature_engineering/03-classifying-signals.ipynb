{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying time series\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tommoral/24-sacl-ai-4-sciences/blob/main/session-3/02-classifying-signals.ipynb)\n",
    "\n",
    "Authors: Thomas Moreau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Cloaked_in_red.jpg/520px-Cloaked_in_red.jpg\" width=\"400\" style=\"display:inline-block;\"/> <span minimum-width=150 style=\"display:inline-block;\"></span><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Pluto-Charon_system-new.gif/440px-Pluto-Charon_system-new.gif\" width=\"350\" style=\"display:inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "## Variable stars\n",
    "\n",
    "Most stars emit light steadily in time, but a small fraction of them has a variable light curve: light emission versus time. We call them variable stars. The light curves are usually periodic and highly regular. There are essentially two reasons why light emission can vary. First, the star itself can be oscillating, so its light emission varies in time. Second, the star that seems a single point at Earth (because of our large distance) is actually a binary system: two stars that orbit around their common center of gravity. When the orbital plane is parallel to our line of view, the stars eclipse each other periodically, creating a light curve with a charateristic signature. Identifying, classifying, and analyzing variable stars are hugely important for calibrating distances, and making these analyses automatic will be crucial in the upcoming sky survey projects such as LSST.\n",
    "\n",
    "In this notebook, we consider an extract of 2912 stars from the [EROS-1 catalog](http://eros.in2p3.fr/) with 4 types of variable light curves:\n",
    "- [Eclipsing binary](http://en.wikipedia.org/wiki/Binary_star#Eclipsing_binaries),\n",
    "- [Cepheid](http://en.wikipedia.org/wiki/Cepheid_variable),\n",
    "- [RR-Lyrae](http://en.wikipedia.org/wiki/RR_Lyrae_variable),\n",
    "- [Mira](http://en.wikipedia.org/wiki/Mira_variable).\n",
    "\n",
    "Our goal is to learn a function that assign an observed variable star to one of these 4 classes. The observations are composed of the light curves in two frequency bands (red and blue), as well as static variables. \n",
    "\n",
    "Let us for load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_FILE = \"data/variable_stars.parquet\"\n",
    "URL_REPO = \"https://github.com/x-datascience-datacamp/datacamp-master/raw/main/session-3/\"\n",
    "\n",
    "if Path(DATA_FILE).exists():\n",
    "    X_df = pd.read_parquet(DATA_FILE)\n",
    "else:\n",
    "    X_df = pd.read_parquet(f\"{URL_REPO}{DATA_FILE}\")\n",
    "\n",
    "y = X_df['type']\n",
    "X_df = X_df.drop(columns='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few informative static variables in this dataframe:\n",
    "\n",
    "<ul>\n",
    "    <li> <code><b><span style=\"color:black\">period</span></b></code>: the estimated period of the light curve.\n",
    "    <li> <code><b>magnitude_b, magnitude_r</b></code>: The average apparent luminosity of the star (in two frequency bands). <a href=\"http://www.astro-tom.com/technical_data/magnitude_scale.htm\">Magnitude</a> is a logarithmic measure, and the higher it is, the lower the apparent luminosity is.\n",
    "    <li> <code><b><span style=\"color:black\">log_p_not_variable</span></b></code>: Logarithm of the estimated probability that the star is stable.\n",
    "    <li> <code><b><span style=\"color:black\">sigma_flux_b, sigma_flux_r</span></b></code>: The square root of the total variance of the light measurements (indicating the amplitude of the variability).\n",
    "    <li> <code><b><span style=\"color:red\">type</span></b></code>: The label to predict.\n",
    "</ul> \n",
    "\n",
    "Let us first start with a simple model based on these static features.\n",
    "We start by splitting the data between train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, random_state=20240707)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start with a dummy method, to see how hard this problem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf = DummyClassifier(\n",
    "    strategy=\"most_frequent\"\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "    Using what you learn yesterday, can you create a pipeline with the following steps\n",
    "     <ul>\n",
    "         <li>A column transformer that selects columns <code>'magnitude_b', 'magnitude_r', 'sigma_flux_b', 'sigma_flux_r', 'log_p_not_variable'</code>,</li>\n",
    "         <li>A <code>RandomForestClassifier</code></li>\n",
    "    </ul>\n",
    "    \n",
    " <i>Hint:</i> To select the columns without modifying them, you can use <code>'passthrough'</code> as a <code>Transformer</code> in the <code>ColumnTransformer</code>.\n",
    "</div>\n",
    "\n",
    "Solution: `solutions/02-1_static-pipeline.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    ...  # to fill\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a classifier based on the light curves\n",
    "\n",
    "We will now turn toward building a classifier based on the raw light curves, that are stored in `time_points_r/b` and `light_points_r/b` columns of our dataset.\n",
    "\n",
    "A first step when working with signals is usually to display them and take a look at their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for idx in [10, 11, 12]:\n",
    "    star = X_df.iloc[idx]\n",
    "    print(f\"Number of time points for star {idx}: {len(star['time_points_b'])}\")\n",
    "\n",
    "    plt.scatter(star['time_points_b'], star['light_points_b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>QUESTION</b>:\n",
    "     <ul>\n",
    "         <li>What can you say about these time-series?\n",
    "             <i>Are they regular? Comparable? Noisy?</i></li>\n",
    "         <li>How would you compare them?</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning and resampling the signals\n",
    "\n",
    "The benefit of working with global features is that they can often be computed even with unaligned time-series.\n",
    "However, these features can fail to catpure some interesting notion of similarity between the samples.\n",
    "\n",
    "We will now explore how to align and resample the data.\n",
    "\n",
    "Here, we have data that is periodic, and we are informed about the period by the column `period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df['period']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus look at the data on one period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for idx in [10, 11, 100]:\n",
    "    star = X_df.iloc[idx]\n",
    "    print(f\"Number of time points for star {idx}: {len(star['time_points_b'])}\")\n",
    "\n",
    "    plt.scatter(star['time_points_b'] % star['period'], star['light_points_b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>How would you modify this code to \"align\" the time series?</li>\n",
    "         <li>How would you modify this code to cope with the fact that the sample are not aligned?</li>\n",
    "    </ul>\n",
    "    \n",
    "   *Hint:* resampling non-uniform points can be performed with the [`scipy.interpolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html) function.\n",
    "</div>\n",
    "\n",
    "Solution: `solutions/02-2_align_periodic_signals.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do a `scikit-learn` pipeline directly on the resampled light curves using the `FunctionTransformer`.\n",
    "`FunctionTransformer` allow you to use any kind of function transforming your data in a `scikit-learn` pipeline.\n",
    "The caveats of this tranformer is that it cannot do anything during the `fit` part of the pipeline.\n",
    "To create a new `Transformer` that resample the data, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "t = np.linspace(0, 1, 100)\n",
    "def resample(star):\n",
    "    t_i = (star['time_points_b'] % star['period']) / star['period']\n",
    "    interp = interpolate.interp1d(\n",
    "        t_i, star['light_points_b'],\n",
    "        kind='linear', bounds_error=False, fill_value=\"extrapolate\"\n",
    "    )\n",
    "    return pd.Series({k: v for k, v in zip(t, interp(t))})\n",
    "\n",
    "X_train_t = FunctionTransformer(lambda X: X.apply(resample, axis=1)).transform(X_train)\n",
    "X_train_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "    Extend your static pipeline with the resampled time-series.\n",
    "</div>\n",
    "\n",
    "Solution: `solutions/02-3_static-pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    ...  # to fill\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a time-series classifiers with `tslearn` and `pyts`\n",
    "\n",
    "We will now cover how to use classifiers adapted to the time-series, using the [`tslearn`](https://tslearn.readthedocs.io/en/stable/gen_modules/neighbors/tslearn.neighbors.KNeighborsTimeSeriesClassifier.html) and [`pyts`](https://pyts.readthedocs.io/en/stable/generated/pyts.classification.TimeSeriesForest.html#pyts.classification.TimeSeriesForest) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tslearn pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "clf = make_pipeline(\n",
    "    FunctionTransformer(lambda X: X.apply(resample, axis=1)),\n",
    "    KNeighborsTimeSeriesClassifier(n_neighbors=5, metric=\"dtw\", n_jobs=-1)\n",
    ").fit(X_train[:100], y_train[:100])\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "\n",
    "clf = make_pipeline(\n",
    "    FunctionTransformer(lambda X: X.apply(resample, axis=1)),\n",
    "    TimeSeriesSVC()\n",
    ").fit(X_train[:100], y_train[:100])\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyts.classification import TimeSeriesForest\n",
    "\n",
    "clf = make_pipeline(\n",
    "    FunctionTransformer(lambda X: X.apply(resample, axis=1)),\n",
    "    TimeSeriesForest(n_windows=5)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these two package implements many algorithms, even for unsupervised learning.\n",
    "They all expect the signals to have the same length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting global features with `tsfresh`\n",
    "\n",
    "\n",
    "`Tsfresh` is a package aimed at automatically extracting global features from time series.\n",
    "It expects as input `pandas.DataFrame`, with a `id` column that indicates to which the signal each row is linked, and a `time` column that indicate the time the sample has been observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this format, it is convenient to represent non-uniform time-series, with variable lenghts.\n",
    "Let us create these `DataFrame` for the train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_train_b = pd.DataFrame([\n",
    "    {'id': idx, 'time': t, 'light_b': v}\n",
    "    for idx, row in X_train.iterrows()\n",
    "    for t, v in zip(row['time_points_b'], row['light_points_b'])\n",
    "])\n",
    "df_ts_train_r = pd.DataFrame([\n",
    "    {'id': idx, 'time': t, 'light_b': v}\n",
    "    for idx, row in X_train.iterrows()\n",
    "    for t, v in zip(row['time_points_r'], row['light_points_r'])\n",
    "])\n",
    "\n",
    "df_ts_test_b = pd.DataFrame([\n",
    "    {'id': idx, 'time': t, 'light_b': v}\n",
    "    for idx, row in X_test.iterrows()\n",
    "    for t, v in zip(row['time_points_b'], row['light_points_b'])\n",
    "])\n",
    "df_ts_test_r = pd.DataFrame([\n",
    "    {'id': idx, 'time': t, 'light_r': v}\n",
    "    for idx, row in X_test.iterrows()\n",
    "    for t, v in zip(row['time_points_r'], row['light_points_r'])\n",
    "])\n",
    "df_ts_train_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tsfresh` provides a convenient helper to extract many possible features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "extracted_features = extract_relevant_features(df_ts_train_b, y_train, column_id=\"id\", column_sort=\"time\")\n",
    "extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They provide a convenient `scikit-learn` like API, that allow working with pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tsfresh.transformers import RelevantFeatureAugmenter\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "clf = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        ('passthrough', ['magnitude_b', 'magnitude_r', 'sigma_flux_b', 'sigma_flux_r', 'log_p_not_variable'])\n",
    "    ),\n",
    "    RelevantFeatureAugmenter(timeseries_container=df_ts_train_b, column_id='id'),\n",
    "    RandomForestClassifier()\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.set_params(relevantfeatureaugmenter__timeseries_container=df_ts_test_b)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering with `tslearn`\n",
    "\n",
    "The `tslearn` library also provides algorithms to do some clustering with the time series using adapted distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import  make_pipeline, FunctionTransformer\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "\n",
    "km = make_pipeline(\n",
    "    FunctionTransformer(lambda X: X.apply(resample, axis=1)),\n",
    "    TimeSeriesScalerMeanVariance(),\n",
    "    TimeSeriesKMeans(\n",
    "        n_clusters=5, metric=\"softdtw\", max_iter=5,\n",
    "        max_iter_barycenter=5, random_state=0\n",
    "    )\n",
    ")\n",
    "y_pred = km.fit_predict(X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for yi in range(5):\n",
    "    plt.subplot(3, 3, yi + 1)\n",
    "    X_t = km[:-1].transform(X_train[:100])\n",
    "    for xx in X_t[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(km[-1].cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, 100)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"Euclidean $k$-means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
